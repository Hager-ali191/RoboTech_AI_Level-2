{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wA-vrPaERaN"
      },
      "source": [
        "# **Cost function**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> Cost Function : a step to minimize the loss or reach the global minimum value \\\n",
        "    \n",
        "    Has more than one type :\n",
        "\n",
        "  >- Mean Absolute Error : Measures how far the predictions are from the actual values on average without considering direction\n",
        "  MAE Equation -> j ( w , b ) = ( 1 / 2m ) * Sum(|yi - yi^|)\n",
        "\n",
        "  >- Mean Square Error : sometimes i need to make the error bigger than actually it is to be more accurate\n",
        "  MSE Equation -> j ( w , b ) = ( 1 / 2m ) * Sum(yi - yi^)^2\n",
        "\n",
        "  >- R^2 (Coefficient Of Determination) : tells how much of the variance in the real data is explained by your model\n",
        "  Equation -> Sum(yi - yi^)^2 / Sum(yi - average(y))^2\n",
        "\n",
        "  where :\n",
        "  \n",
        "  - i -> as an index from 1 to m \\\n",
        "  - y^ -> wx + b \\\n",
        "  - m -> number of samples\n",
        "---"
      ],
      "metadata": {
        "id": "zACpdohM2QjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIb7JjlM9IbI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def cost_function(x, y, w, b):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "              to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = x.shape[0]\n",
        "\n",
        "    #TODO: compute the cost function result\n",
        "    # y^ = wx + b\n",
        "    prediction= w*x+b\n",
        "\n",
        "    total_cost= np.sum((prediction - y)**2)\n",
        "\n",
        "    #END OF CODE\n",
        "    # MSE\n",
        "    return total_cost/(2*m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FbSu5jIFA-L"
      },
      "source": [
        "# **Gradient/Derivative terms**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "how do we minimize the loss ?\n",
        "\n",
        "> Gradient Descent : Uses derivatives to update the weights(w) and bias(b) in order to minimize the loss \\\n",
        "\\\n",
        "    - start with random w , b \\\n",
        "    - keep changing them to reduce j(w , b) -> (cost function) \\\n",
        "    - end up at the minimum\n",
        "\n",
        "  >w = w0 - alpha * d/dw j(w , b) \\\n",
        "  d/dw = (1 / m) * Sum(yi - yi^) * x\n",
        "\n",
        "  >b = b0 - alpha * d/db j(w , b) \\\n",
        "  d/db = (1 / m) * Sum(yi - yi^)\n",
        "\n",
        ">alpha : learning rate that controls how big each step is then updating w , b during training \\\n",
        "If TOO BIG , results overshooting and hardly reach the minimum \\\n",
        "If TOO SMALL , very slow training & might take too many epochs to learn anything \\\n",
        "To Choose a good learning rate try : 0.001 , 0.01 , 0.1 , 1 , ... \\\n",
        "\n",
        ">If (Derivative * alpha) < 0 , It is in the left side of the minimum point so we should go right \\\n",
        "If (Derivative * alpha) > 0 , It is in the right side of the minimum point so we should go left \\\n",
        "If (Derivative * alpha) = 0 , Stop Updating\n",
        "---"
      ],
      "metadata": {
        "id": "fopuar-W1jV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Xpn6ZhFJgc"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(x, y, w, b):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
        "     \"\"\"\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "\n",
        "    # Number of training examples\n",
        "    m = x.shape[0]\n",
        "\n",
        "    #TODO: compute the gradients\n",
        "    # y^ = wx + b\n",
        "    prediction= w*x+b\n",
        "\n",
        "    # error = y^ - y\n",
        "    errors= prediction - y\n",
        "\n",
        "    # derivatives\n",
        "    dj_dw = (1/m) * np.dot(errors, x)\n",
        "    dj_dw = (1/m) * np.sum(errors * x)\n",
        "\n",
        "    dj_db = (1/m) * np.sum(errors)\n",
        "\n",
        "    #END OF CODE\n",
        "\n",
        "    return dj_dw, dj_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2NzdFJRGF7F"
      },
      "source": [
        "# **Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI6YsrdsGJCo"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,))  : Data, m examples\n",
        "      y (ndarray (m,))  : target values\n",
        "      w_in,b_in (scalar): initial values of model parameters\n",
        "      alpha (float):      Learning rate\n",
        "      num_iters (int):    number of iterations to run gradient descent\n",
        "      dj_dw, dj_db:       The gradienta\n",
        "\n",
        "    Returns:\n",
        "      w (scalar): Updated value of parameter after running gradient descent\n",
        "      b (scalar): Updated value of parameter after running gradient descent\n",
        "      \"\"\"\n",
        "\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "\n",
        "    #TODO: update the weights using gradient descent algorithm\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
        "\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "    #END OF CODE\n",
        "\n",
        "    return w, b #return w and b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Polynomial Regression**"
      ],
      "metadata": {
        "id": "FKVHHN1q0O53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">Equation : y^ = w0 + w1 * x + w2 * (x ^ 2) + ... + wn * (x ^ 2) \\\n",
        "or we can reduce the degree : w2 * root(x) , ... \\\n",
        "when data can not be fitted by a line so we need a curved line\n",
        "---"
      ],
      "metadata": {
        "id": "m7wX9i0x0Blx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiple Regression**\n"
      ],
      "metadata": {
        "id": "BCGnrTBM0sw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">Equation : y^ = w0 + w1 * x1 + w2 * x2 + ... + wn * xn \\\n",
        "If I will convert it as two vectors then will start x with x0 = 1 \\\n",
        "and will be W(transpose) * x or Vise Versa\n",
        "---"
      ],
      "metadata": {
        "id": "XJHJQKdt0c3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Steps of Regression**"
      ],
      "metadata": {
        "id": "v26k0uSn0_9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">1 - initialize w , b with random values & choose learning rate \\\n",
        "2 - use the initialized w , b to predict the output \\\n",
        "3 - calculate cost function\\\n",
        "4 - calculate the gradient \\\n",
        "5 - update w , b \\\n",
        "6 - repeat form 2 to 5 until converge to the minimum or achieve maximum iterations\n",
        "---\n"
      ],
      "metadata": {
        "id": "c3ssZdp01FpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bias & Variance**"
      ],
      "metadata": {
        "id": "CREqNaYh3SYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variance** :\n",
        "- error due to **too much complexity** in algorithm\n",
        "- leads to the algorithm being **highly sensitive** to **high degrees** of variation in your training data , which can lead to **overfit**\n",
        "\n"
      ],
      "metadata": {
        "id": "I1xwSZMo3WeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bias** :\n",
        "- error due to overly **simplistic assumptions** in algorithm\n",
        "- leads the model to **underfit** , making it **hard** for it to have **high predictive accuracy** and for you to **generalize your knowledge** from the training set to the test set"
      ],
      "metadata": {
        "id": "xlDnKJzj4LCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Regularization**"
      ],
      "metadata": {
        "id": "RTlKA0aN45kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Technique used to penalize large coefficients (weights) in a regression model\n",
        "- add a penalty term to the cost function\n",
        "- without it , model can become too complex , fitting training data perfectly but performing poorly on new unseen data , which is overfitting"
      ],
      "metadata": {
        "id": "isAOdQK34-y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 Regularization : Lasso Regression**\n",
        "\n",
        "- Equation : y^ = Cost Function + ( (lamda / m) * Sum(|Wj|))\n",
        "\n",
        "- Effect :\n",
        "  - Forces some weights to become exactly zero\n",
        "  - Performs feature selection automatically\n",
        "  - Useful when you suspect that only a few features are important"
      ],
      "metadata": {
        "id": "M-Hhydz_5iny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 Regularization : Ridge Regression**\n",
        "\n",
        "- Equation : y^ = Cost Function + ( (lamda / m) * Sum( (Wj) ^ 2 ))\n",
        "\n",
        "- Effect :\n",
        "  - Shrinks weights towards zero but never exactly zero\n",
        "  - Reduces model complexity\n",
        "  - Smooths the fit\n",
        "  - Useful when many features are relevant"
      ],
      "metadata": {
        "id": "J40SX5PW6fnO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-wA-vrPaERaN",
        "6FbSu5jIFA-L",
        "S2NzdFJRGF7F",
        "FKVHHN1q0O53",
        "BCGnrTBM0sw9",
        "v26k0uSn0_9O",
        "CREqNaYh3SYX",
        "RTlKA0aN45kZ"
      ]
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
